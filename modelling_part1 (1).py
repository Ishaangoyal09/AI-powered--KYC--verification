# -*- coding: utf-8 -*-
"""Modelling_part1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QNFI4oFKOD18tuzt-kpIPAaQvTJtiBzW
"""

# Import Libraries
import pandas as pd
import re
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns
!pip install fuzzywuzzy python-Levenshtein
from fuzzywuzzy import fuzz

# Load CSV (only first 500 rows)
file_path = "/content/Image_Verification_Results.csv"

df = pd.read_csv(file_path).head(2000)
print("✅ Data loaded successfully! Shape:", df.shape)
print(df.head())

"""#  Data Cleaning & Preprocessing

---


 Removing any kind of special character and loweracsing all the letters to find similarity between two rows later on.
"""

# Clean Name data
df['Name_clean'] = df['Name'].str.lower().apply(lambda x: re.sub(r'[^a-z\s]', '', str(x)))

# Clean Address Data
df['Address_clean'] = df['Address'].str.lower().apply(lambda x: re.sub(r'[^a-z0-9\s]', '', str(x)))
df.head()

# Clean Gender data
df['Gender_clean'] = df['Gender'].str.lower().apply(lambda x: re.sub(r'[^a-z\s]', '', str(x)))
df.head()

# Clean Document_Type
df['Document_type_clean'] = df['Document_Type'].str.lower().apply(lambda x: re.sub(r'[^a-z0-9\s]', '', str(x)))
df.head()

# ✅ Fill all NaN values in the DataFrame with empty strings
df = df.fillna('')

# ✅ Safely handle the 'DOB' column
if 'DOB' not in df.columns:
    df['DOB'] = 'Unknown'  # Create it if missing
else:
    df['DOB'] = df['DOB'].replace('', 'Unknown')

# ✅ Safely handle the 'Document_Number' column
if 'Document_Number' not in df.columns:
    df['Document_Number'] = 'Missing'
else:
    df['Document_Number'] = df['Document_Number'].replace('', 'Missing')

df.head()

"""#  Document Validation (Aadhaar / PAN  /Passport)

Validating document numbers with their unique format.
Example: adhaar has 12 no, pan has mix of alphanumeric and numeric letter.
"""

def validate_document(doc_num, doc_type):
    doc_num = str(doc_num).strip()
    doc_type = str(doc_type).lower() # Convert to string and then to lower case
    if doc_type == 'aadhaar':
        return bool(re.match(r'^\d{12}$', doc_num))
    elif doc_type == 'pan':
        return bool(re.match(r'^[A-Z]{5}[0-9]{4}[A-Z]$', doc_num))
    elif doc_type == 'passport':
        return bool(re.match(r'^[A-Z]{1}[0-9]{7}$', doc_num))
    else:
        return False

df['Document_format'] = df.apply(lambda x: validate_document(x['Document_Number'], x['Document_Type']), axis=1)
df.head()

"""# Combining all important field into one single text
Merging all the important distinct data together to form a single text, which will help in similarity detection.Every letter is lowercased but Keeping Document_Number as it is because it is a case sensitive and unique attribute
"""

df["Combined_Data"] = (
    df["Name_clean"].astype(str) + " " +
    df["Gender_clean"].astype(str) + " " +
    df["DOB"].astype(str) + " " +
    df["Document_type_clean"].astype(str) + " " +
    df["Document_Number"].astype(str) + " " +  # keep case-sensitive
    df["Address_clean"].astype(str)
).str.replace(r"[^a-zA-Z0-9 ]", "", regex=True)  # keep letters in original case for Document_Number

df.head()

"""# Similarity Checking
It checks how similar two KYC records are, based on their combined text (Combined_Data column, which includes name, DOB, address, doc type, and doc number).
If two rows have high similarity (say, > 80%), it marks them as potential duplicates.
"""

from fuzzywuzzy import fuzz

# Similarity Score Between Records
similarity_scores = []

for i in range(len(df)):
    for j in range(i + 1, len(df)):
        score = fuzz.token_sort_ratio(df.loc[i, 'Combined_Data'], df.loc[j, 'Combined_Data'])
        if score > 80:  # threshold can be tuned (e.g. 80–90)
            similarity_scores.append({
                'Record_1': i,
                'Record_2': j,
                'Similarity': score
            })

similarity_df = pd.DataFrame(similarity_scores)
print("✅ Potential duplicate or similar records found:", similarity_df.shape[0])
display(similarity_df.head(10))

"""# Dealing with duplicate records:

***group_mapping*** only contains indices of rows that were found similar (connected components).

***df.index.map(group_mapping)*** maps each row’s index to its group ID if it exists in the mapping.

Rows not present in group_mapping (i.e. no duplicates found) get a NaN.

***.fillna(-1)*** replaces that NaN with -1, meaning “no duplicate group / unique record”.

So:

Group_ID = 0, 1, 2, ... → Similar or duplicate clusters.

Group_ID = -1 → Record is unique, not flagged as duplicate.
"""

import networkx as nx

# Build graph where nodes are records and edges connect similar ones
G = nx.Graph()
for _, row in similarity_df.iterrows():
    G.add_edge(row['Record_1'], row['Record_2'])

# Assign group IDs
group_mapping = {}
for group_id, cluster in enumerate(nx.connected_components(G)):
    for index in cluster:
        group_mapping[index] = group_id

df['Duplicate_Group_ID'] = df.index.map(group_mapping).fillna(-1).astype(int)

df.head()

"""#Fraud Heuristics

Adding a new column that gives a Fraud_Risk_Score based on:

-Invalid document number

-Missing DOB

-Duplicate or high-similarity entries






"""

def compute_fraud_score(row):
    score = 0
    if not row['Document_format']:
        score += 0.4
    if pd.isna(row['DOB']):
        score += 0.3
    if row['Duplicate_Group_ID'] != -1:
        score += 0.3
    return round(score, 2)

df['Fraud_Risk_Score'] = df.apply(compute_fraud_score, axis=1)

def risk_label(score):
    if score < 0.2:
        return 'Low'
    elif score < 0.433:  # Lowered threshold
        return 'Medium'
    else:
        return 'High'   # Now High = score >= 0.5


df['Fraud_Risk_Level'] = df['Fraud_Risk_Score'].apply(risk_label)

df.head(10)

# visualisation of risk level

plt.figure(figsize=(2,2))
sns.countplot(x='Fraud_Risk_Level', data=df, palette='Set2')
plt.title("Fraud Risk Distribution")
plt.show()
# counting the no of low,mid and high in fraud risk level
count1 = (df['Fraud_Risk_Level'] == 'Low').sum()  # safer, works even if value not present
count2 = (df['Fraud_Risk_Level'] == 'Medium').sum()
count3 = (df['Fraud_Risk_Level'] == 'High').sum()
print(count1,count2,count3)

#  Clean Combined Data (avoid “nan” in text)
df["Combined_Data"] = df[[
    "Name_clean", "Gender_clean", "DOB", "Document_type_clean", "Document_Number", "Address_clean"
]].apply(lambda x: ' '.join(x.fillna('missing').astype(str)), axis=1)

df.head()

# Now save final cleaned version
df.to_csv("/content/untitled1.csv", index=False)
print("✅ Final processed file saved as: kyc_final_processed.csv (empty cells handled)")

df.head()